"""
LLMæœåŠ¡æ¨¡å—
è´Ÿè´£ä¸å„å¤§è¯­è¨€æ¨¡å‹APIçš„äº¤äº’
"""

import json
import os
import ssl
import urllib.request
import urllib.error
from typing import Optional, List, Dict
from PySide6.QtCore import QObject, Signal, QThread


class LLMWorker(QThread):
    """LLMè¯·æ±‚å·¥ä½œçº¿ç¨‹"""

    result_ready = Signal(str, bool, str)  # (request_id, success, result/error)

    def __init__(self, request_id: str, model_name: str, config: dict,
                 messages: List[Dict], system_prompt: str, max_tokens: int = 500):
        super().__init__()
        self.request_id = request_id
        self.model_name = model_name
        self.config = config
        self.messages = messages
        self.system_prompt = system_prompt
        self.max_tokens = max_tokens

    def run(self):
        """æ‰§è¡ŒAPIè°ƒç”¨"""
        try:
            result = self._call_api()
            self.result_ready.emit(self.request_id, True, result)
        except Exception as e:
            self.result_ready.emit(self.request_id, False, str(e))

    def _call_api(self) -> str:
        """è°ƒç”¨å…·ä½“çš„API"""
        api_key = self.config.get("api_key", "")
        base_url = self.config.get("base_url", "")
        model = self.config.get("model", "")

        if not api_key:
            raise ValueError("APIå¯†é’¥æœªé…ç½®")

        # æ ¹æ®ä¸åŒæ¨¡å‹è°ƒç”¨ä¸åŒAPI
        if self.model_name == "ChatGPT":
            return self._call_openai(api_key, base_url, model)
        elif self.model_name == "Gemini":
            return self._call_gemini(api_key, base_url, model)
        elif self.model_name == "é˜¿é‡Œåƒé—®":
            return self._call_qwen(api_key, base_url, model)
        elif self.model_name == "DeepSeek":
            return self._call_deepseek(api_key, base_url, model)
        elif self.model_name == "kimi":
            return self._call_kimi(api_key, base_url, model)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {self.model_name}")

    def _call_openai(self, api_key: str, base_url: str, model: str) -> str:
        """è°ƒç”¨OpenAI API"""
        url = f"{base_url}/chat/completions"

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": self.system_prompt},
                *self.messages
            ],
            "temperature": 0.7,
            "max_tokens": self.max_tokens
        }

        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode('utf-8'),
            headers=headers,
            method='POST'
        )

        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯ä¹¦éªŒè¯
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        with urllib.request.urlopen(req, timeout=60, context=ssl_context) as response:
            data = json.loads(response.read().decode('utf-8'))
            return data['choices'][0]['message']['content']

    def _call_gemini(self, api_key: str, base_url: str, model: str) -> str:
        """è°ƒç”¨Gemini API"""
        url = f"{base_url}/v1beta/models/{model}:generateContent?key={api_key}"

        # æ„å»ºå¯¹è¯å†å²
        contents = []
        for msg in self.messages:
            role = "user" if msg["role"] == "user" else "model"
            contents.append({"role": role, "parts": [{"text": msg["content"]}]})

        payload = {
            "contents": contents,
            "generationConfig": {
                "temperature": 0.7,
                "maxOutputTokens": self.max_tokens
            }
        }

        headers = {"Content-Type": "application/json"}

        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode('utf-8'),
            headers=headers,
            method='POST'
        )

        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯ä¹¦éªŒè¯
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        with urllib.request.urlopen(req, timeout=60, context=ssl_context) as response:
            data = json.loads(response.read().decode('utf-8'))
            if 'candidates' in data and data['candidates']:
                return data['candidates'][0]['content']['parts'][0]['text']
            raise ValueError("Gemini APIè¿”å›æ ¼å¼é”™è¯¯")

    def _call_qwen(self, api_key: str, base_url: str, model: str) -> str:
        """è°ƒç”¨é˜¿é‡Œåƒé—®API"""
        url = f"{base_url}/api/v1/services/aigc/text-generation/generation"

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

        # æ„å»ºæç¤ºè¯
        prompt = self.system_prompt + "\n\n"
        for msg in self.messages:
            role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
            prompt += f"{role}: {msg['content']}\n"
        prompt += "åŠ©æ‰‹: "

        payload = {
            "model": model,
            "input": {"prompt": prompt},
            "parameters": {
                "temperature": 0.7,
                "max_tokens": self.max_tokens
            }
        }

        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode('utf-8'),
            headers=headers,
            method='POST'
        )

        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯ä¹¦éªŒè¯
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        with urllib.request.urlopen(req, timeout=60, context=ssl_context) as response:
            data = json.loads(response.read().decode('utf-8'))
            return data['output']['text']

    def _call_deepseek(self, api_key: str, base_url: str, model: str) -> str:
        """è°ƒç”¨DeepSeek API"""
        url = f"{base_url}/chat/completions"

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": self.system_prompt},
                *self.messages
            ],
            "temperature": 0.7,
            "max_tokens": self.max_tokens
        }

        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode('utf-8'),
            headers=headers,
            method='POST'
        )

        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯ä¹¦éªŒè¯
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        with urllib.request.urlopen(req, timeout=60, context=ssl_context) as response:
            data = json.loads(response.read().decode('utf-8'))
            return data['choices'][0]['message']['content']

    def _call_kimi(self, api_key: str, base_url: str, model: str) -> str:
        """è°ƒç”¨Kimi API"""
        url = f"{base_url}/chat/completions"

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": self.system_prompt},
                *self.messages
            ],
            "temperature": 0.7,
            "max_tokens": self.max_tokens
        }

        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode('utf-8'),
            headers=headers,
            method='POST'
        )

        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯ä¹¦éªŒè¯
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        with urllib.request.urlopen(req, timeout=60, context=ssl_context) as response:
            data = json.loads(response.read().decode('utf-8'))
            return data['choices'][0]['message']['content']


class LLMService(QObject):
    """LLMæœåŠ¡ï¼Œç®¡ç†å¤§è¯­è¨€æ¨¡å‹çš„è°ƒç”¨"""

    reply_ready = Signal(str, str)      # (request_id, reply_text)
    error_occurred = Signal(str, str)   # (request_id, error_message)

    def __init__(self, config_manager):
        super().__init__()
        self.config_manager = config_manager
        self._workers: Dict[str, LLMWorker] = {}
        self._system_prompt = """ä½ æ˜¯è‰¾è€å„¿å‡å‘çš„é©¬è€å¸ˆåŠ©ç†ï¼ŒæœåŠ¡ä¸­è€å¹´å¥³æ€§å®¢æˆ·ç¾¤ä½“ã€‚ä½ çš„å®šä½æ˜¯"é©¬è€å¸ˆç§äººå‘å‹åŠ©ç†"ï¼Œä¸åªæ˜¯å–å‡å‘ï¼Œæ›´æ˜¯ä¸ºå®¢æˆ·é‡èº«å®šåˆ¶è‡ªä¿¡ã€‚

ã€é‡è¦æç¤ºã€‘
- ä½ ä¼šæ”¶åˆ°å®Œæ•´çš„èŠå¤©è®°å½•ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©ä½ ç†è§£å®¢æˆ·çš„å…³æ³¨ç‚¹å’Œéœ€æ±‚
- èŠå¤©è®°å½•ä»…ä¾›å‚è€ƒï¼Œç”¨äºåˆ†æå®¢æˆ·æ„å›¾ï¼Œä¸è¦é‡å¤å›å¤å·²ç»è¯´è¿‡çš„å†…å®¹
- ä½ åªéœ€è¦é’ˆå¯¹å®¢æˆ·çš„æœ€æ–°æ¶ˆæ¯å›å¤ä¸€æ¬¡ï¼Œä¸è¦åˆ†æ®µå›å¤ï¼Œä¸è¦é‡å¤å›å¤
- æ¯æ¬¡åªç”Ÿæˆä¸€æ¡å®Œæ•´çš„å›å¤æ¶ˆæ¯

ã€å“ç‰Œå®šä½ã€‘
å“ç‰Œåï¼šè‰¾è€å„¿å‡å‘
æ ¸å¿ƒç†å¿µï¼šä¸åªæ˜¯å–å‡å‘ï¼Œæ›´æ˜¯ä¸ºæ‚¨é‡èº«å®šåˆ¶è‡ªä¿¡
å“ç‰Œå£å·ï¼šæ‚¨çš„ç§äººå‘å‹é¡¾é—®ï¼Œè®©ç¾ä¸½ä»å¤´å¼€å§‹

ã€æœåŠ¡åŸåˆ™ã€‘
1. ä¸“ä¸šå¯é ï¼šæ‡‚å‡å‘çŸ¥è¯†ï¼Œèƒ½è§£ç­”å„ç§æŠ€æœ¯ã€æè´¨ã€æŠ¤ç†é—®é¢˜
2. è€å¿ƒç»†è‡´ï¼šç†è§£å®¢æˆ·çš„ç„¦è™‘ï¼ˆå°¤å…¶æ˜¯é¦–æ¬¡è´­ä¹°æˆ–å› è„±å‘ç­‰é—®é¢˜ï¼‰ï¼Œä¸åŒå…¶çƒ¦
3. æ¸©æš–å…±æƒ…ï¼šèƒ½ä½“ä¼šå®¢æˆ·å¯¹ç¾ä¸½å’Œè‡ªä¿¡çš„æ¸´æœ›ï¼Œè¯­è¨€å……æ»¡å…³æ€€å’Œé¼“åŠ±
4. æ—¶å°šæ•é”ï¼šå¯¹å‘å‹ã€æ½®æµã€è„¸å‹æ­é…æœ‰ç‹¬åˆ°è§è§£

ã€äº§å“ä¿¡æ¯ã€‘
æŠ€æœ¯ï¼šæ—¥æœ¬ä¸“åˆ©æŠ€æœ¯ï¼Œå¸‚åœºç‹¬å®¶
æè´¨ï¼šçœŸäººå‘ä¸+100%çº¯æ‰‹å·¥è‰º
æ ¸å¿ƒç‰¹ç‚¹ï¼š
- è®°å¿†å‘ä¸ï¼Œå…æ‰“ç†
- æ¸å˜ä»¿çœŸå¤´çš®æŠ€æœ¯
- æ¤ç‰©ç½‘åº•+é€æ°”å­”è®¾è®¡
- æ ¹æ®å¤´å›´ã€è„¸å‹ã€è‚¤è‰²ç§äººå®šåˆ¶
ä½¿ç”¨å¯¿å‘½ï¼šä¿å…»å¾—å½“å¯ä½¿ç”¨3-5å¹´
å”®åæœåŠ¡ï¼šç»ˆèº«ä¿å…»æœåŠ¡
ä»·æ ¼åŒºé—´ï¼š3000ã€5000  5000ã€6000éƒ½æœ‰
åˆ¶ä½œå‘¨æœŸï¼šçº¦2ä¸ªæœˆ

ã€é—¨åº—ä¿¡æ¯ã€‘
åŒ—äº¬ï¼šä»…1å®¶çº¿ä¸‹é—¨åº—ï¼Œæœé˜³åŒºå»ºå¤–SOHO
ä¸Šæµ·ï¼šä»…5ä¸ªå¯æ¨èåŒºåŸŸï¼Œå¾æ±‡åŒºã€é™å®‰åŒºã€äººæ°‘å¹¿åœºã€è™¹å£ã€äº”è§’åœº

ã€åœ°ç†ä½ç½®åˆ†æµè§„åˆ™ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰ã€‘
- ç”¨æˆ·åœ¨ä¸Šæµ·æˆ–æ±Ÿæµ™æ²ªå‘¨è¾¹ï¼ˆå¦‚è‹å·ã€æ— é”¡ã€å¸¸å·ã€å—é€šã€å—äº¬ã€å®æ³¢ã€æ­å·ã€ç»å…´ã€å˜‰å…´ã€æ¹–å·ç­‰ï¼‰ï¼Œä¼˜å…ˆæ¨èä¸Šæµ·é—¨åº—
- ç”¨æˆ·åœ¨åŒ—äº¬æˆ–åŒ—äº¬å‘¨è¾¹ï¼ˆå¦‚å¤©æ´¥ã€æ²³åŒ—ï¼‰ï¼Œä¼˜å…ˆæ¨èåŒ—äº¬æœé˜³åŒºå»ºå¤–SOHO
- ä¸Šæµ·åŒºçº§æ¨èå£å¾„ï¼š
  * å¦‚æœç”¨æˆ·è¯´åœ¨é—µè¡ŒåŒº -> æ¨èå¥¹å»æˆ‘ä»¬å¾æ±‡åŒºé—¨åº—
  * å¦‚æœç”¨æˆ·è¯´åœ¨é•¿å®åŒº -> æ¨èå¥¹å»æˆ‘ä»¬é™å®‰åŒºé—¨åº—
  * å¦‚æœç”¨æˆ·è¯´åœ¨æ¨æµ¦åŒº/äº”è§’åœº -> æ¨èå¥¹å»æˆ‘ä»¬äº”è§’åœºé—¨åº—
  * å¦‚æœç”¨æˆ·è¯´åœ¨é»„æµ¦åŒº/äººæ°‘å¹¿åœº/äººå¹¿ -> æ¨èå¥¹å»æˆ‘ä»¬äººæ°‘å¹¿åœºé—¨åº—
  * å…¶ä»–ä¸Šæµ·åŒº -> æ¨èå¥¹å»æˆ‘ä»¬äººæ°‘å¹¿åœºé—¨åº—
- ç”¨æˆ·åœ°åŒºä¸åœ¨å·²çŸ¥é—¨åº—è¦†ç›–æ—¶ï¼Œä¸å¯ç¼–é€ é—¨åº—ï¼›å…ˆè¯¢é—®ç”¨æˆ·æ‰€åœ¨åŒº/åŸå¸‚ï¼Œå†ä»å·²çŸ¥é—¨åº—é‡Œæ¨èæœ€è¿‘åŒºåŸŸ

ã€é—¨åº—çœŸå®æ€§çº¦æŸï¼ˆä¸¥ç¦èƒ¡ä¹±å›å¤ï¼‰ã€‘
- ç¦æ­¢è™šæ„ä»»ä½•â€œæœ¬åœ°é—¨åº—/æœåŠ¡ç‚¹/åˆ†åº—â€
- ç¦æ­¢è¯´â€œåŒ—äº¬æœ‰å¤šå®¶é—¨åº—â€
- é—¨å¤´æ²Ÿç­‰åŒ—äº¬å…¶ä»–åŒºä¹Ÿåªèƒ½å¼•å¯¼åˆ°æœé˜³åŒºå»ºå¤–SOHOï¼Œä¸èƒ½è¯´è¯¥åŒºæœ‰é—¨åº—
- å›ç­”é—¨åº—æ—¶åªèƒ½åœ¨ä»¥ä¸Šå·²çŸ¥èŒƒå›´å†…è¡¨è¾¾
- è‹¥ä¸Šæ¸¸æ¶ˆæ¯é‡Œç»™å‡º `target_store`ï¼Œå¿…é¡»æŒ‰è¯¥é—¨åº—è¡¨è¾¾ï¼Œä¸èƒ½æ”¹å†™ä¸ºå…¶å®ƒé—¨åº—

ã€æ²Ÿé€šè§„èŒƒã€‘
ç§°å‘¼ï¼šç»Ÿä¸€ä½¿ç”¨"å§å§"ï¼ˆäº²åˆ‡ç§°å‘¼ï¼‰
è¯­æ°”ï¼šæ¸©æš–ã€ä¸“ä¸šã€è€å¿ƒã€å…±æƒ…
å›å¤é•¿åº¦ï¼š30-80å­—ï¼Œç®€æ´æ˜äº†ï¼Œè¨€ç®€æ„èµ…
è¡¨æƒ…è¦æ±‚ï¼šæ¯æ¡å›å¤å¿…é¡»åŒ…å«1ä¸ªemojiï¼Œä¸”å°½é‡éšæœºä½¿ç”¨ï¼Œä¸è¦æ€»æ˜¯åŒä¸€ä¸ª
emojiå‚è€ƒï¼šğŸ˜ŠğŸŒ¸âœ¨ğŸ‘ğŸ¤ğŸ’—ğŸ˜„ğŸ™‚ğŸ˜‰ğŸ¥°ğŸ™ŒğŸ’ªğŸŒŸğŸ€ğŸ«¶ğŸ˜ŒğŸ¤—ğŸ’«ğŸŒ·ğŸ‰
å›å¤æ¬¡æ•°ï¼šåªå›å¤ä¸€æ¬¡ï¼Œä¸è¦åˆ†æ®µ
å¸¸ç”¨è¯æœ¯ï¼š
- "æ‚¨æ”¾å¿ƒ"
- "æˆ‘å»ºè®®"
- "ä¸ºæ‚¨"

ã€å¹³å°åˆè§„ï¼ˆå¿…é¡»éµå®ˆï¼‰ã€‘
- ä¸¥ç¦è¾“å‡ºæˆ–æš—ç¤ºä»»ä½•è”ç³»æ–¹å¼ï¼šå¾®ä¿¡ã€å¾®ä¿¡å·ã€è”ç³»ç”µè¯ã€ç”µè¯ã€æ‰‹æœºå·ã€QQã€äºŒç»´ç ã€å¤–é“¾ã€é‚®ç®±ç­‰
- å³ä½¿å®¢æˆ·ä¸»åŠ¨ç´¢è¦è”ç³»æ–¹å¼ï¼Œä¹Ÿä¸èƒ½æä¾›ï¼Œæ”¹ä¸ºå¼•å¯¼åœ¨å½“å‰å¹³å°å†…ç»§ç»­æ²Ÿé€š
- è‹¥è‰ç¨¿ä¸­å‡ºç°ä¸Šè¿°å†…å®¹ï¼Œå¿…é¡»è‡ªåŠ¨æ”¹å†™ä¸ºåˆè§„è¡¨è¾¾åå†è¾“å‡º

ã€å¯¹è¯ç­–ç•¥ã€‘

1. é¦–æ¬¡å’¨è¯¢å®¢æˆ·ï¼š
- å…ˆåˆ¤æ–­æ˜¯å¦ç¬¬ä¸€æ¬¡è´­ä¹°å‡å‘
- ç¬¬ä¸€æ¬¡è´­ä¹°ï¼šé‡ç‚¹å¼•å¯¼åˆ°åº—ä½“éªŒï¼Œå¼ºè°ƒ"ä¹°ä¸ä¹°æ²¡å…³ç³»ï¼Œå…ˆè®©é€ å‹å¸ˆçœ‹çœ‹é€‚åˆä»€ä¹ˆé€ å‹"
- æœ‰è´­ä¹°ç»éªŒï¼šè¯¢é—®ä¹‹å‰æ˜¯çº¿ä¸Šè¿˜æ˜¯çº¿ä¸‹è´­ä¹°ï¼Œé’ˆå¯¹æ€§è¯´æ˜æˆ‘ä»¬çš„ä¼˜åŠ¿
- è‹¥ç”¨æˆ·å°šæœªæ˜ç¡®æ‰€åœ¨åŸå¸‚/åŒºåŸŸï¼Œå…ˆç®€çŸ­ä»‹ç»äº§å“ä¸æœåŠ¡ï¼Œå†è¿½é—®â€œå§å§æ‚¨å¤§æ¦‚åœ¨å“ªä¸ªåŒº/åŸå¸‚å‘¢ï¼Ÿâ€ï¼›ä¸è¦å…ˆé»˜è®¤æ¨èåŒ—äº¬æˆ–ä¸Šæµ·

2. ä»·æ ¼å¼‚è®®å¤„ç†ï¼š
- ä¸è¦ç›´æ¥æŠ¥ä»·ï¼Œå…ˆäº†è§£éœ€æ±‚
- å¦‚æœå®¢æˆ·è§‰å¾—è´µï¼Œä½¿ç”¨ä»¥ä¸‹è¯æœ¯ï¼š
  * æ—¥å‡æˆæœ¬æ³•ï¼š"7000å…ƒå‡å‘ç”¨3-5å¹´ï¼Œæ¯å¤©ä¸åˆ°3.8å…ƒï¼Œæ¯”é¢‘ç¹æ›´æ¢å»‰ä»·å‡å‘æ›´åˆ’ç®—"
  * è´¨é‡å¯¹æ¯”ï¼š"æ™®é€šåŒ–çº¤å‡å‘3ä¸ªæœˆå°±è¦æ¢ï¼Œæˆ‘ä»¬çš„çœå¿ƒåˆçœé’±"
  * å®¢æˆ·é€‰æ‹©ï¼š"å¾ˆå¤šé¡¾å®¢æ˜çŸ¥é“æˆ‘ä»¬ä»·æ ¼è´µä¸€ç‚¹ï¼Œæœ€åè¿˜æ˜¯é€‰æ‹©æˆ‘ä»¬ï¼Œå› ä¸ºä»–ä»¬åœ¨ä½ä»·å’Œå“è´¨ä¹‹é—´åšå‡ºäº†æ­£ç¡®çš„é€‰æ‹©"
- å¦‚æœé—®æœ€ä½ä»·ï¼š"ç°åœ¨å¸‚åœºè¿™ä¹ˆæ¿€çƒˆï¼Œèƒ½ç»™ä½ å°‘100ï¼Œæˆ‘ç»å¯¹ä¸ä¼šç»™ä½ å°‘ä¸€åˆ†ï¼Œæˆ‘çš„ç›®çš„å°±æ˜¯ç•™ä¸‹ä½ ï¼Œåˆæ€ä¹ˆä¼šå‚»åˆ°æŠ¥é«˜ä»·æŠŠä½ å¾€å¤–æ¨å‘¢ï¼Ÿ"

3. å¸¸è§é¡¾è™‘å¤„ç†ï¼š
- å¤´å‘ä¼šæ‰å—ï¼š"éå¸¸ç‰¢å›ºï¼Œæˆ‘ä»¬æœ‰å®¢æˆ·æˆ´ç€åšè¿‡å±±è½¦éƒ½æ²¡é—®é¢˜"
- å¤©æ°”çƒ­ï¼š"æ¤ç‰©ç½‘åº•+é€æ°”å­”è®¾è®¡ï¼Œå¤å¤©æˆ´ä¹Ÿä¸ä¼šé—·æ±—ï¼Œçˆ±è·³å¹¿åœºèˆçš„å§å§åé¦ˆè¯´æˆ´ä¸Šæ²¡ä»€ä¹ˆæ„Ÿè§‰è·ŸçœŸå‘ä¸€æ ·ã€‚"
- æ‹…å¿ƒä¸è‡ªç„¶ï¼š"çœŸäººå‘ä¸+100%çº¯æ‰‹å·¥è‰ºï¼Œå‘æ ¹éƒ½æ˜¯ä»¿çœŸå¤´çš®æ•ˆæœï¼Œæˆ´ä¸Šå»å’ŒçœŸå‘ä¸€æ ·å¾ˆè‡ªç„¶"
- çº¿ä¸Šå‡å‘ä¾¿å®œï¼š"çº¿ä¸Šå‡å‘è™½ç„¶ä¾¿å®œä½†è´¨é‡è‰¯è ä¸é½ï¼Œæˆ´ä¸äº†ä¸¤ä¸‰ä¸ªæœˆã€‚å’±ä»¬å‡å‘æ ¹æ®å¤´å›´ã€è„¸å‹ç§äººå®šåˆ¶ï¼Œç»ˆèº«ä¿å…»ï¼Œæˆ´3-5å¹´è´¨é‡éƒ½ä¸ä¼šæœ‰é—®é¢˜"

4. æˆäº¤æŠ€å·§ï¼š
- "ä¹°ä¸ä¹°éƒ½æ²¡å…³ç³»ï¼Œä½†æˆ‘å‘ç»™ä½ çš„èµ„æ–™å¯ä»¥è®©ä½ åœ¨é€‰æ‹©äº§å“çš„æ—¶å€™å°‘èµ°å¼¯è·¯"
- "æ‚¨çš„æƒ…å†µçœŸçš„éå¸¸éœ€è¦æˆ‘ä»¬çš„äº§å“"
- "å§å§ï¼Œä½ è§‰å¾—æ˜¯è¿™ä¸ªæ–¹æ¡ˆæ›´é€‚åˆä½ ï¼Œè¿˜æ˜¯æ›´ä¸­æ„è¿™ä¸ªæ–¹æ¡ˆå‘¢ï¼Ÿ"ï¼ˆäºŒé€‰ä¸€æˆäº¤æ³•ï¼‰

ã€å›å¤è¦ç‚¹ã€‘
1. æ ¹æ®èŠå¤©è®°å½•åˆ†æå®¢æˆ·å¤„äºå“ªä¸ªé˜¶æ®µï¼ˆåˆæ¬¡å’¨è¯¢/ä»·æ ¼å¼‚è®®ç­‰ï¼‰
2. é’ˆå¯¹å®¢æˆ·çš„å…·ä½“é—®é¢˜æˆ–é¡¾è™‘ï¼Œä½¿ç”¨å¯¹åº”çš„è¯æœ¯
3. æ¯æ¬¡å›å¤éƒ½è¦æœ‰æ˜ç¡®çš„å¼•å¯¼ç›®æ ‡ï¼ˆäº†è§£éœ€æ±‚â†’è§£å†³é¡¾è™‘ï¼‰
4. ä¿æŒæ¸©æš–å…±æƒ…çš„è¯­æ°”ï¼Œè®©å®¢æˆ·æ„Ÿå—åˆ°è¢«ç†è§£å’Œé‡è§†
5. é€‚æ—¶ä½¿ç”¨"ä¹°ä¸ä¹°æ²¡å…³ç³»"é™ä½å®¢æˆ·å‹åŠ›"""

    def generate_reply(self, user_message: str, conversation_history: List[Dict] = None,
                       request_id: str = None) -> str:
        """ç”Ÿæˆå›å¤

        Args:
            user_message: ç”¨æˆ·æœ€æ–°æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²
            request_id: è¯·æ±‚IDï¼ˆç”¨äºè¿½è¸ªï¼‰

        Returns:
            request_id
        """
        import uuid
        request_id = request_id or str(uuid.uuid4())

        # è·å–å½“å‰æ¨¡å‹é…ç½®
        model_name = self.config_manager.get_current_model()
        model_config = self.config_manager.get_model_config(model_name)

        if not model_config.get("api_key"):
            self.error_occurred.emit(request_id, f"{model_name} çš„APIå¯†é’¥æœªé…ç½®")
            return request_id

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if conversation_history:
            messages.extend(conversation_history)
        messages.append({"role": "user", "content": user_message})

        # åˆ›å»ºå·¥ä½œçº¿ç¨‹
        worker = LLMWorker(
            request_id=request_id,
            model_name=model_name,
            config=model_config,
            messages=messages,
            system_prompt=self._system_prompt
        )

        worker.result_ready.connect(self._on_worker_result)
        self._workers[request_id] = worker

        worker.start()
        return request_id

    def _on_worker_result(self, request_id: str, success: bool, result: str):
        """å¤„ç†å·¥ä½œçº¿ç¨‹ç»“æœ"""
        # æ¸…ç†å·¥ä½œçº¿ç¨‹ - ç­‰å¾…çº¿ç¨‹å®Œæˆåå†åˆ é™¤
        if request_id in self._workers:
            worker = self._workers[request_id]
            if worker.isRunning():
                worker.wait()  # ç­‰å¾…çº¿ç¨‹å®Œæˆ
            del self._workers[request_id]

        if success:
            self.reply_ready.emit(request_id, result)
        else:
            self.error_occurred.emit(request_id, result)

    def set_system_prompt(self, prompt: str):
        """è®¾ç½®ç³»ç»Ÿæç¤ºè¯"""
        self._system_prompt = prompt

    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰å·¥ä½œçº¿ç¨‹"""
        for request_id, worker in list(self._workers.items()):
            if worker.isRunning():
                worker.quit()
                worker.wait(1000)  # ç­‰å¾…æœ€å¤š1ç§’
        self._workers.clear()

    def get_system_prompt(self) -> str:
        """è·å–å½“å‰ç³»ç»Ÿæç¤ºè¯"""
        return self._system_prompt

    def test_connection(self, model_name: str = None) -> tuple:
        """æµ‹è¯•æ¨¡å‹è¿æ¥

        Returns:
            (success: bool, message: str)
        """
        model_name = model_name or self.config_manager.get_current_model()
        config = self.config_manager.get_model_config(model_name)

        if not config.get("api_key"):
            return False, "APIå¯†é’¥æœªé…ç½®"

        if not config.get("base_url"):
            return False, "APIåœ°å€æœªé…ç½®"

        try:
            # å‘é€æœ€å°æµ‹è¯•è¯·æ±‚ï¼ˆå¯èƒ½äº§ç”Ÿå°‘é‡tokenæ¶ˆè€—ï¼‰
            worker = LLMWorker(
                request_id="test",
                model_name=model_name,
                config=config,
                messages=[{"role": "user", "content": "ping"}],
                system_prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹",
                max_tokens=1
            )
            worker._call_api()  # ç›´æ¥è°ƒç”¨ä»¥æ•è·é‰´æƒ/è¿æ¥é”™è¯¯
            return True, "è¿æ¥æˆåŠŸ"
        except Exception as e:
            return False, f"è¿æ¥å¤±è´¥: {str(e)}"

    def cancel_request(self, request_id: str):
        """å–æ¶ˆè¯·æ±‚"""
        if request_id in self._workers:
            worker = self._workers[request_id]
            if worker.isRunning():
                worker.terminate()
            del self._workers[request_id]

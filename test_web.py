"""
LLMæµ‹è¯•WebæœåŠ¡
ç‹¬ç«‹è¿è¡Œçš„Flaskåº”ç”¨ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•å¤§æ¨¡å‹å›å¤é€»è¾‘
å®Œæ•´è¿˜åŸé¡¹ç›®é€»è¾‘ï¼šæ¨¡æ‹Ÿæ¶ˆæ¯æŠ“å– -> çŸ¥è¯†åº“åŒ¹é… -> LLMå›å¤
"""

import os
import sys
import uuid
import time
import json
import shutil
from pathlib import Path
from datetime import datetime
from threading import Event, Lock
from flask import Flask, request, jsonify, send_file, Response

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.data.config_manager import ConfigManager
from src.data.knowledge_repository import KnowledgeRepository
from src.services.knowledge_service import KnowledgeService
from src.services.llm_service import LLMService
from src.core.reply_coordinator import ReplyCoordinator
from src.core.session_manager import SessionManager

app = Flask(__name__)

# å…¨å±€æœåŠ¡å®ä¾‹
_config_manager = None
_knowledge_service = None
_knowledge_repository = None
_llm_service = None
_reply_coordinator = None
_session_manager = None

# è¯·æ±‚å–æ¶ˆç®¡ç†
_pending_requests = {}
_cancel_flag = Lock()

# å¯¹è¯å†å²ï¼ˆå†…å­˜ä¸­ä¿å­˜ï¼‰
_conversation_history = {}

# ç”¨æˆ·å›å¤å»é‡ç¼“å­˜
_user_reply_cache = {}  # {user_hash: set(reply_hash)}
_reply_cache_lock = Lock()

# æ—¥å¿—é˜Ÿåˆ—
_logs = []
_logs_lock = Lock()
MAX_LOGS = 500

# ä¸Šä¼ å›¾ç‰‡å­˜å‚¨ç›®å½•
UPLOAD_DIR = PROJECT_ROOT / "test_uploads"
UPLOAD_DIR.mkdir(exist_ok=True)

# å›¾ç‰‡ç›®å½•ï¼ˆä¸ä¸»é¡¹ç›®ä¸€è‡´ï¼‰
IMAGES_DIR = PROJECT_ROOT / "images"
IMAGES_DIR.mkdir(exist_ok=True)


def add_log(message: str, level: str = "info"):
    """æ·»åŠ æ—¥å¿—"""
    with _logs_lock:
        timestamp = datetime.now().strftime("%H:%M:%S")
        _logs.append({
            "time": timestamp,
            "message": message,
            "level": level
        })
        if len(_logs) > MAX_LOGS:
            _logs.pop(0)
    print(f"[{timestamp}] {message}")


def check_and_optimize_reply(user_name: str, user_message: str, original_reply: str) -> tuple:
    """æ£€æŸ¥å›å¤æ˜¯å¦é‡å¤ï¼Œå¦‚æœé‡å¤åˆ™è®©å¤§æ¨¡å‹ä¼˜åŒ–
    
    Args:
        user_name: ç”¨æˆ·å
        user_message: ç”¨æˆ·æ¶ˆæ¯
        original_reply: åŸå§‹å›å¤
    
    Returns:
        (optimized_reply, is_duplicate, source)
    """
    import hashlib
    
    # ç”Ÿæˆç”¨æˆ·æ ‡è¯†ï¼ˆåŸºäºç”¨æˆ·åï¼‰
    user_hash = hashlib.md5(user_name.encode()).hexdigest()[:8]
    
    # ç”Ÿæˆå›å¤å†…å®¹çš„å“ˆå¸Œå€¼
    reply_hash = hashlib.md5(original_reply.encode()).hexdigest()[:8]
    
    with _reply_cache_lock:
        # åˆå§‹åŒ–ç”¨æˆ·ç¼“å­˜
        if user_hash not in _user_reply_cache:
            _user_reply_cache[user_hash] = set()
        
        # æ£€æŸ¥æ˜¯å¦é‡å¤
        if reply_hash in _user_reply_cache[user_hash]:
            add_log(f"ğŸ”„ æ£€æµ‹åˆ°é‡å¤å›å¤ï¼Œè®©å¤§æ¨¡å‹ä¼˜åŒ–è¯æœ¯...")
            add_log(f"ğŸ“ åŸå§‹å›å¤: {original_reply[:50]}...")
            
            # æ„é€ ä¼˜åŒ–æç¤º
            optimize_prompt = f"""è¯·ä¼˜åŒ–ä»¥ä¸‹å®¢æœå›å¤ï¼Œè¦æ±‚ï¼š
1. ä¿æŒæ ¸å¿ƒä¿¡æ¯ä¸å˜
2. æ”¹å˜è¡¨è¾¾æ–¹å¼å’Œå¥å¼ç»“æ„
3. é¿å…ä¸ä¹‹å‰å›å¤é‡å¤
4. ä¿æŒæ¸©æš–ä¸“ä¸šçš„è¯­æ°”
5. é•¿åº¦æ§åˆ¶åœ¨30-80å­—

ç”¨æˆ·é—®é¢˜ï¼š{user_message}
åŸå§‹å›å¤ï¼š{original_reply}

è¯·æä¾›ä¼˜åŒ–åçš„å›å¤ï¼š"""
            
            # è°ƒç”¨å¤§æ¨¡å‹ä¼˜åŒ–
            optimized_reply, error = call_llm_directly(
                user_message=optimize_prompt,
                conversation_history=[]
            )
            
            if error:
                add_log(f"âŒ ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›å¤: {error}", "error")
                return original_reply, True, "çŸ¥è¯†åº“(é‡å¤)"
            
            if optimized_reply and optimized_reply.strip():
                add_log(f"âœ¨ å¤§æ¨¡å‹ä¼˜åŒ–æˆåŠŸ: {optimized_reply[:50]}...")
                # è®°å½•ä¼˜åŒ–åçš„å›å¤
                optimized_hash = hashlib.md5(optimized_reply.encode()).hexdigest()[:8]
                _user_reply_cache[user_hash].add(optimized_hash)
                return optimized_reply, True, "å¤§æ¨¡å‹(ä¼˜åŒ–)"
            else:
                add_log(f"âŒ å¤§æ¨¡å‹è¿”å›ç©ºå›å¤ï¼Œä½¿ç”¨åŸå§‹å›å¤", "error")
                return original_reply, True, "çŸ¥è¯†åº“(é‡å¤)"
        else:
            # é¦–æ¬¡å›å¤ï¼Œè®°å½•åˆ°ç¼“å­˜
            _user_reply_cache[user_hash].add(reply_hash)
            add_log(f"ğŸ†• é¦–æ¬¡å›å¤ï¼Œè®°å½•åˆ°ç¼“å­˜")
            return original_reply, False, "åŸå§‹"


def call_llm_directly(user_message: str, conversation_history: list = None) -> tuple:
    """ç›´æ¥è°ƒç”¨LLM APIï¼Œä¸ä¾èµ–Qtä¿¡å·æœºåˆ¶"""
    try:
        # è·å–å½“å‰æ¨¡å‹é…ç½®
        model_name = _config_manager.get_current_model()
        model_config = _config_manager.get_model_config(model_name)
        
        add_log(f"ğŸ“¡ ä½¿ç”¨æ¨¡å‹: {model_name}")
        add_log(f"ğŸ”— APIåœ°å€: {model_config.get('base_url', '')}")
        
        api_key = model_config.get("api_key", "")
        base_url = model_config.get("base_url", "")
        model = model_config.get("model", "")
        
        if not api_key:
            return None, "APIå¯†é’¥æœªé…ç½®"
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if conversation_history:
            messages.extend(conversation_history)
        messages.append({"role": "user", "content": user_message})
        
        # è·å–ç³»ç»Ÿprompt
        system_prompt = _llm_service.get_system_prompt()
        add_log(f"ğŸ“ ç³»ç»Ÿprompté•¿åº¦: {len(system_prompt)}")
        
        # æ ¹æ®æ¨¡å‹è°ƒç”¨ä¸åŒAPI
        if model_name == "é˜¿é‡Œåƒé—®":
            return _call_qwen_direct(api_key, base_url, model, messages, system_prompt)
        elif model_name == "DeepSeek":
            return _call_deepseek_direct(api_key, base_url, model, messages, system_prompt)
        elif model_name == "ChatGPT":
            return _call_openai_direct(api_key, base_url, model, messages, system_prompt)
        else:
            return None, f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}"
            
    except Exception as e:
        add_log(f"âŒ LLMè°ƒç”¨å¼‚å¸¸: {str(e)}", "error")
        return None, f"LLMè°ƒç”¨å¼‚å¸¸: {str(e)}"


def _call_qwen_direct(api_key: str, base_url: str, model: str, messages: list, system_prompt: str) -> tuple:
    """ç›´æ¥è°ƒç”¨é˜¿é‡Œåƒé—®API"""
    import json
    import ssl
    import urllib.request
    
    try:
        url = f"{base_url}/api/v1/services/aigc/text-generation/generation"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        # æ„å»ºæç¤ºè¯ï¼ˆåƒé—®æ ¼å¼ï¼‰
        prompt = system_prompt + "\n\n"
        for msg in messages:
            role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
            prompt += f"{role}: {msg['content']}\n"
        prompt += "åŠ©æ‰‹: "
        
        payload = {
            "model": model,
            "input": {"prompt": prompt},
            "parameters": {
                "temperature": 0.7,
                "max_tokens": 500
            }
        }
        
        add_log(f"ğŸ“¤ å‘é€è¯·æ±‚åˆ°åƒé—®API...")
        
        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode('utf-8'),
            headers=headers,
            method='POST'
        )
        
        # åˆ›å»ºSSLä¸Šä¸‹æ–‡
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        with urllib.request.urlopen(req, timeout=30, context=ssl_context) as response:
            data = json.loads(response.read().decode('utf-8'))
            
            if 'output' in data and 'text' in data['output']:
                result = data['output']['text']
                add_log(f"âœ… åƒé—®APIè°ƒç”¨æˆåŠŸ")
                return result, None
            else:
                error_msg = f"åƒé—®APIè¿”å›æ ¼å¼é”™è¯¯: {data}"
                add_log(f"âŒ {error_msg}", "error")
                return None, error_msg
                
    except urllib.error.HTTPError as e:
        error_msg = f"åƒé—®API HTTPé”™è¯¯ {e.code}: {e.reason}"
        add_log(f"âŒ {error_msg}", "error")
        return None, error_msg
    except Exception as e:
        error_msg = f"åƒé—®APIè°ƒç”¨å¼‚å¸¸: {str(e)}"
        add_log(f"âŒ {error_msg}", "error")
        return None, error_msg


def _call_deepseek_direct(api_key: str, base_url: str, model: str, messages: list, system_prompt: str) -> tuple:
    """ç›´æ¥è°ƒç”¨DeepSeek API"""
    import json
    import ssl
    import urllib.request
    
    try:
        url = f"{base_url}/chat/completions"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                *messages
            ],
            "temperature": 0.7,
            "max_tokens": 500
        }
        
        add_log(f"ğŸ“¤ å‘é€è¯·æ±‚åˆ°DeepSeek API...")
        
        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode('utf-8'),
            headers=headers,
            method='POST'
        )
        
        # åˆ›å»ºSSLä¸Šä¸‹æ–‡
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        with urllib.request.urlopen(req, timeout=30, context=ssl_context) as response:
            data = json.loads(response.read().decode('utf-8'))
            
            if 'choices' in data and len(data['choices']) > 0:
                result = data['choices'][0]['message']['content']
                add_log(f"âœ… DeepSeek APIè°ƒç”¨æˆåŠŸ")
                return result, None
            else:
                error_msg = f"DeepSeek APIè¿”å›æ ¼å¼é”™è¯¯: {data}"
                add_log(f"âŒ {error_msg}", "error")
                return None, error_msg
                
    except urllib.error.HTTPError as e:
        error_msg = f"DeepSeek API HTTPé”™è¯¯ {e.code}: {e.reason}"
        add_log(f"âŒ {error_msg}", "error")
        return None, error_msg
    except Exception as e:
        error_msg = f"DeepSeek APIè°ƒç”¨å¼‚å¸¸: {str(e)}"
        add_log(f"âŒ {error_msg}", "error")
        return None, error_msg


def _call_openai_direct(api_key: str, base_url: str, model: str, messages: list, system_prompt: str) -> tuple:
    """ç›´æ¥è°ƒç”¨OpenAI API"""
    import json
    import ssl
    import urllib.request
    
    try:
        url = f"{base_url}/chat/completions"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                *messages
            ],
            "temperature": 0.7,
            "max_tokens": 500
        }
        
        add_log(f"ğŸ“¤ å‘é€è¯·æ±‚åˆ°OpenAI API...")
        
        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode('utf-8'),
            headers=headers,
            method='POST'
        )
        
        # åˆ›å»ºSSLä¸Šä¸‹æ–‡
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        with urllib.request.urlopen(req, timeout=30, context=ssl_context) as response:
            data = json.loads(response.read().decode('utf-8'))
            
            if 'choices' in data and len(data['choices']) > 0:
                result = data['choices'][0]['message']['content']
                add_log(f"âœ… OpenAI APIè°ƒç”¨æˆåŠŸ")
                return result, None
            else:
                error_msg = f"OpenAI APIè¿”å›æ ¼å¼é”™è¯¯: {data}"
                add_log(f"âŒ {error_msg}", "error")
                return None, error_msg
                
    except urllib.error.HTTPError as e:
        error_msg = f"OpenAI API HTTPé”™è¯¯ {e.code}: {e.reason}"
        add_log(f"âŒ {error_msg}", "error")
        return None, error_msg
    except Exception as e:
        error_msg = f"OpenAI APIè°ƒç”¨å¼‚å¸¸: {str(e)}"
        add_log(f"âŒ {error_msg}", "error")
        return None, error_msg


def init_services():
    """åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡"""
    global _config_manager, _knowledge_service, _knowledge_repository, _llm_service, _reply_coordinator, _session_manager
    
    # é…ç½®æ–‡ä»¶è·¯å¾„ - ä½¿ç”¨æ­£ç¡®çš„æ–‡ä»¶å
    config_file = PROJECT_ROOT / "config" / "model_settings.json"
    knowledge_file = PROJECT_ROOT / "config" / "knowledge_base.json"
    
    # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
    _config_manager = ConfigManager(config_file=config_file, env_file=None)
    
    # åˆå§‹åŒ–çŸ¥è¯†åº“
    _knowledge_repository = KnowledgeRepository(knowledge_file)
    _knowledge_service = KnowledgeService(_knowledge_repository)
    
    # åˆå§‹åŒ–ä¼šè¯ç®¡ç†å™¨
    _session_manager = SessionManager()
    
    # åˆå§‹åŒ–LLMæœåŠ¡
    _llm_service = LLMService(_config_manager)
    
    # åˆå§‹åŒ–å›å¤åè°ƒå™¨ï¼ˆä¸åŸé¡¹ç›®ä¿æŒä¸€è‡´ï¼‰
    _reply_coordinator = ReplyCoordinator(
        knowledge_service=_knowledge_service,
        llm_service=_llm_service,
        session_manager=_session_manager
    )
    
    add_log("âœ… æœåŠ¡åˆå§‹åŒ–å®Œæˆ", "success")
    add_log(f"ğŸ“‹ å½“å‰æ¨¡å‹: {_config_manager.get_current_model()}")
    add_log(f"ğŸ“š çŸ¥è¯†åº“æ¡ç›®: {_knowledge_service.get_count()}")


@app.route('/')
def index():
    """è¿”å›æµ‹è¯•é¡µé¢"""
    return send_file(PROJECT_ROOT / "test_chat.html")


@app.route('/api/logs', methods=['GET'])
def get_logs():
    """è·å–æ—¥å¿—"""
    since = int(request.args.get("since", 0))
    with _logs_lock:
        return jsonify({"logs": _logs[since:]})


@app.route('/api/config', methods=['GET'])
def get_config():
    """è·å–å½“å‰é…ç½®"""
    return jsonify({
        "current_model": _config_manager.get_current_model(),
        "available_models": ["é˜¿é‡Œåƒé—®", "DeepSeek", "ChatGPT", "Gemini", "kimi"],
        "knowledge_count": _knowledge_service.get_count(),
        "system_prompt": _llm_service.get_system_prompt()
    })


@app.route('/api/config', methods=['POST'])
def update_config():
    """æ›´æ–°é…ç½®"""
    data = request.json
    
    if "current_model" in data:
        model_name = data["current_model"]
        _config_manager.set_current_model(model_name)
        add_log(f"ğŸ”„ åˆ‡æ¢æ¨¡å‹: {model_name}")
    
    if "system_prompt" in data:
        _llm_service.set_system_prompt(data["system_prompt"])
        add_log("ğŸ“ æ›´æ–°ç³»ç»Ÿæç¤ºè¯")
    
    return jsonify({"success": True})


@app.route('/api/chat', methods=['POST'])
def chat():
    """å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤ - å®Œå…¨è¿˜åŸåŸé¡¹ç›®é€»è¾‘"""
    data = request.json
    user_message = data.get("message", "").strip()
    session_id = data.get("session_id", "default")
    use_knowledge = data.get("use_knowledge", True)
    user_name = data.get("user_name", "æµ‹è¯•ç”¨æˆ·")
    
    if not user_message:
        return jsonify({"error": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º"}), 400
    
    start_time = time.time()
    
    # æ¨¡æ‹Ÿæ¶ˆæ¯æŠ“å–æ—¥å¿—
    add_log("=" * 50)
    add_log(f"ğŸ“‹ ç”¨æˆ·èŠå¤©è®°å½•ï¼š{user_name}")
    add_log("=" * 50)
    
    # è·å–æˆ–åˆ›å»ºä¼šè¯ï¼ˆä½¿ç”¨åŸé¡¹ç›®çš„ä¼šè¯ç®¡ç†å™¨ï¼‰
    session = _session_manager.get_or_create_session(session_id, user_name)
    
    # è®°å½•ç”¨æˆ·æ¶ˆæ¯åˆ°ä¼šè¯
    _session_manager.add_message(session_id, user_message, is_user=True)
    
    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    history = session.get_conversation_history(6)  # æœ€è¿‘6è½®å¯¹è¯
    for msg in history:
        if msg.get("is_user"):
            add_log(f"â¤ï¸â€ğŸ”¥ ç”¨æˆ·ï¼ˆ{user_name}ï¼‰ï¼š{msg['content'][:50]}...")
        else:
            add_log(f"ğŸ¤– å®¢æœï¼ˆæˆ‘ï¼‰ï¼š{msg['content'][:50]}...")
    
    # æ˜¾ç¤ºå½“å‰æ¶ˆæ¯
    add_log(f"â¤ï¸â€ğŸ”¥ ç”¨æˆ·ï¼ˆ{user_name}ï¼‰ï¼š{user_message}")
    add_log("=" * 50)
    add_log(f"ğŸ’¬ [{user_name}]: {user_message[:50]}...")
    
    # ä½¿ç”¨å›å¤åè°ƒå™¨å¤„ç†æ¶ˆæ¯ï¼ˆä¸åŸé¡¹ç›®å®Œå…¨ä¸€è‡´ï¼‰
    reply_text = None
    error_msg = None
    reply_event = Event()
    request_id = None
    
    def on_reply(success: bool, reply: str):
        nonlocal reply_text, error_msg
        if success and reply:
            reply_text = reply
        else:
            error_msg = reply or "ç”Ÿæˆå›å¤å¤±è´¥"
        reply_event.set()
        # æ¸…ç†å¾…å¤„ç†è¯·æ±‚
        with _cancel_flag:
            if request_id in _pending_requests:
                del _pending_requests[request_id]
    
    # ç”Ÿæˆå”¯ä¸€è¯·æ±‚ID
    import uuid
    request_id = str(uuid.uuid4())
    
    # è®°å½•å¾…å¤„ç†è¯·æ±‚
    with _cancel_flag:
        _pending_requests[request_id] = {
            "session_id": session_id,
            "user_message": user_message,
            "reply_event": reply_event
        }
    
    try:
        # é¦–å…ˆæµ‹è¯•çŸ¥è¯†åº“åŒ¹é…
        add_log(f"ğŸ” æ­£åœ¨åŒ¹é…çŸ¥è¯†åº“...")
        kb_answer = _knowledge_service.find_answer(user_message, threshold=0.6)
        if kb_answer:
            add_log(f"âœ… çŸ¥è¯†åº“åŒ¹é…æˆåŠŸï¼ŒåŸå§‹å›å¤: {kb_answer[:50]}...")
            
            # æ£€æŸ¥å»é‡å¹¶ä¼˜åŒ–
            final_reply, is_duplicate, reply_source = check_and_optimize_reply(
                user_name, user_message, kb_answer
            )
            
            reply_text = final_reply
            actual_source = reply_source
        else:
            add_log(f"âŒ çŸ¥è¯†åº“æœªåŒ¹é…ï¼Œè°ƒç”¨å¤§æ¨¡å‹...")
            
            # ç›´æ¥è°ƒç”¨LLM API
            reply_text, error_msg = call_llm_directly(
                user_message=user_message,
                conversation_history=history[-6:] if history else []
            )
            
            if error_msg:
                add_log(f"âŒ å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {error_msg}", "error")
                return jsonify({"error": error_msg}), 500
            
            if not reply_text:
                add_log(f"âŒ å¤§æ¨¡å‹è¿”å›ç©ºå›å¤", "error")
                return jsonify({"error": "å¤§æ¨¡å‹è¿”å›ç©ºå›å¤"}), 500
            
            add_log(f"ğŸ¤– å¤§æ¨¡å‹å›å¤: {reply_text[:50]}...")
            actual_source = "å¤§æ¨¡å‹"
        
        # è®°å½•åˆ°æœ¬åœ°å†å²ï¼ˆç”¨äºç•Œé¢æ˜¾ç¤ºï¼‰
        if session_id not in _conversation_history:
            _conversation_history[session_id] = []
        
        _conversation_history[session_id].append({"role": "user", "content": user_message})
        _conversation_history[session_id].append({"role": "assistant", "content": reply_text})
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(_conversation_history[session_id]) > 20:
            _conversation_history[session_id] = _conversation_history[session_id][-20:]
        
        add_log(f"âœ… å›å¤å·²å‘é€ (æ¥æº: {actual_source}): {reply_text[:50]}...")
        
        # ç¡®å®šè¿”å›ç»™å‰ç«¯çš„sourceç±»å‹
        if actual_source == "å¤§æ¨¡å‹(ä¼˜åŒ–)":
            source_type = "llm"
        elif actual_source == "çŸ¥è¯†åº“(é‡å¤)":
            source_type = "knowledge"
        elif actual_source == "åŸå§‹":
            source_type = "knowledge"
        else:
            source_type = actual_source
        
        return jsonify({
            "reply": reply_text,
            "source": source_type,
            "actual_source": actual_source,  # æ·»åŠ å®é™…æ¥æºä¿¡æ¯
            "model": _config_manager.get_current_model(),
            "time_ms": int((time.time() - start_time) * 1000)
        })
    
    except Exception as e:
        add_log(f"âŒ å¤„ç†æ¶ˆæ¯å¼‚å¸¸: {str(e)}", "error")
        return jsonify({"error": f"å¤„ç†å¼‚å¸¸: {str(e)}"}), 500
    finally:
        # æ¸…ç†å¾…å¤„ç†è¯·æ±‚
        with _cancel_flag:
            if request_id and request_id in _pending_requests:
                del _pending_requests[request_id]


@app.route('/api/knowledge', methods=['GET'])
def get_knowledge():
    """è·å–çŸ¥è¯†åº“æ•°æ®"""
    try:
        items = _knowledge_repository.get_all()
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        items_dict = [item.to_dict() for item in items]
        return jsonify({"items": items_dict})
    except Exception as e:
        add_log(f"âŒ è·å–çŸ¥è¯†åº“å¤±è´¥: {str(e)}", "error")
        return jsonify({"error": str(e)}), 500


@app.route('/api/knowledge', methods=['POST'])
def add_knowledge():
    """æ·»åŠ çŸ¥è¯†åº“é¡¹"""
    try:
        data = request.json
        
        # åˆ›å»ºæ–°çŸ¥è¯†åº“é¡¹
        item = _knowledge_repository.add(
            question=data['question'],
            answer=data['answer'],
            category=data.get('category', ''),
            tags=data.get('tags', [])
        )
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        _knowledge_repository.save()
        
        add_log(f"âœ… æ·»åŠ çŸ¥è¯†åº“é¡¹: {data['question'][:30]}...", "success")
        return jsonify({"success": True, "item": item.to_dict()})
        
    except Exception as e:
        add_log(f"âŒ æ·»åŠ çŸ¥è¯†åº“é¡¹å¤±è´¥: {str(e)}", "error")
        return jsonify({"error": str(e)}), 500


@app.route('/api/knowledge/<item_id>', methods=['PUT'])
def update_knowledge(item_id):
    """æ›´æ–°çŸ¥è¯†åº“é¡¹"""
    try:
        data = request.json
        
        # è·å–ç°æœ‰é¡¹
        item = _knowledge_repository.get_item(item_id)
        if not item:
            return jsonify({"error": "çŸ¥è¯†åº“é¡¹ä¸å­˜åœ¨"}), 404
        
        # æ›´æ–°é¡¹
        item.question = data['question']
        item.answer = data['answer']
        item.category = data.get('category', '')
        item.tags = data.get('tags', [])
        item.updated_at = datetime.now().isoformat()
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        _knowledge_repository.save()
        
        add_log(f"âœ… æ›´æ–°çŸ¥è¯†åº“é¡¹: {data['question'][:30]}...", "success")
        return jsonify({"success": True, "item": item.to_dict()})
        
    except Exception as e:
        add_log(f"âŒ æ›´æ–°çŸ¥è¯†åº“é¡¹å¤±è´¥: {str(e)}", "error")
        return jsonify({"error": str(e)}), 500


@app.route('/api/knowledge/<item_id>', methods=['DELETE'])
def delete_knowledge(item_id):
    """åˆ é™¤çŸ¥è¯†åº“é¡¹"""
    try:
        # åˆ é™¤é¡¹
        success = _knowledge_repository.delete_item(item_id)
        if not success:
            return jsonify({"error": "çŸ¥è¯†åº“é¡¹ä¸å­˜åœ¨"}), 404
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        _knowledge_repository.save()
        
        add_log(f"âœ… åˆ é™¤çŸ¥è¯†åº“é¡¹: {item_id}", "success")
        return jsonify({"success": True})
        
    except Exception as e:
        add_log(f"âŒ åˆ é™¤çŸ¥è¯†åº“é¡¹å¤±è´¥: {str(e)}", "error")
        return jsonify({"error": str(e)}), 500


@app.route('/api/knowledge/export', methods=['GET'])
def export_knowledge():
    """å¯¼å‡ºçŸ¥è¯†åº“"""
    try:
        items = _knowledge_repository.get_all()
        
        # åˆ›å»ºå¯¼å‡ºæ•°æ®
        export_data = {
            "version": 1,
            "exported_at": datetime.now().isoformat(),
            "items": [item.to_dict() for item in items]
        }
        
        add_log(f"âœ… å¯¼å‡ºçŸ¥è¯†åº“: {len(items)} æ¡è®°å½•", "success")
        
        # è¿”å›JSONæ–‡ä»¶
        from flask import Response
        return Response(
            json.dumps(export_data, ensure_ascii=False, indent=2),
            mimetype='application/json',
            headers={'Content-Disposition': 'attachment; filename=knowledge_base.json'}
        )
        
    except Exception as e:
        add_log(f"âŒ å¯¼å‡ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}", "error")
        return jsonify({"error": str(e)}), 500


@app.route('/api/knowledge/import', methods=['POST'])
def import_knowledge():
    """å¯¼å…¥çŸ¥è¯†åº“"""
    try:
        if 'file' not in request.files:
            return jsonify({"error": "æ²¡æœ‰æ–‡ä»¶"}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({"error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}), 400
        
        if not file.filename.endswith('.json'):
            return jsonify({"error": "åªæ”¯æŒJSONæ–‡ä»¶"}), 400
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = file.read().decode('utf-8')
        data = json.loads(content)
        
        # å¯¼å…¥æ•°æ®
        imported_count = 0
        if 'items' in data:
            for item_data in data['items']:
                _knowledge_repository.add(
                    question=item_data.get('question', ''),
                    answer=item_data.get('answer', ''),
                    category=item_data.get('category', ''),
                    tags=item_data.get('tags', [])
                )
                imported_count += 1
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        _knowledge_repository.save()
        
        add_log(f"âœ… å¯¼å…¥çŸ¥è¯†åº“: {imported_count} æ¡è®°å½•", "success")
        return jsonify({"success": True, "imported": imported_count})
        
    except Exception as e:
        add_log(f"âŒ å¯¼å…¥çŸ¥è¯†åº“å¤±è´¥: {str(e)}", "error")
        return jsonify({"error": str(e)}), 500


@app.route('/api/chat/cancel', methods=['POST'])
def cancel_chat():
    """å–æ¶ˆå½“å‰è¿›è¡Œä¸­çš„èŠå¤©è¯·æ±‚"""
    try:
        # å–æ¶ˆæ‰€æœ‰å¾…å¤„ç†è¯·æ±‚
        with _cancel_flag:
            if _pending_requests:
                add_log(f"ğŸ›‘ æ­£åœ¨å–æ¶ˆ {len(_pending_requests)} ä¸ªå¾…å¤„ç†è¯·æ±‚...")
                for req_id, req_info in list(_pending_requests.items()):
                    # è®¾ç½®äº‹ä»¶ä»¥è§£é™¤ç­‰å¾…
                    req_info["reply_event"].set()
                _pending_requests.clear()
                add_log("âœ… æ‰€æœ‰è¯·æ±‚å·²å–æ¶ˆ")
                return jsonify({"success": True, "message": "è¯·æ±‚å·²å–æ¶ˆ"})
            else:
                return jsonify({"success": True, "message": "æ²¡æœ‰å¾…å¤„ç†çš„è¯·æ±‚"})
    except Exception as e:
        add_log(f"âŒ å–æ¶ˆè¯·æ±‚å¤±è´¥: {str(e)}", "error")
        return jsonify({"error": f"å–æ¶ˆå¤±è´¥: {str(e)}"}), 500


@app.route('/api/history', methods=['GET'])
def get_history():
    """è·å–å¯¹è¯å†å²"""
    session_id = request.args.get("session_id", "default")
    history = _conversation_history.get(session_id, [])
    return jsonify({"history": history})


@app.route('/api/cache/clear', methods=['POST'])
def clear_reply_cache():
    """æ¸…ç©ºå›å¤ç¼“å­˜"""
    try:
        with _reply_cache_lock:
            _user_reply_cache.clear()
        add_log(f"ğŸ—‘ï¸ å›å¤ç¼“å­˜å·²æ¸…ç©º")
        return jsonify({"success": True})
    except Exception as e:
        add_log(f"âŒ æ¸…ç©ºç¼“å­˜å¤±è´¥: {str(e)}", "error")
        return jsonify({"error": str(e)}), 500


@app.route('/api/cache/stats', methods=['GET'])
def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    try:
        with _reply_cache_lock:
            stats = {
                "total_users": len(_user_reply_cache),
                "total_replies": sum(len(replies) for replies in _user_reply_cache.values()),
                "user_details": {}
            }
            
            for user_hash, replies in _user_reply_cache.items():
                stats["user_details"][user_hash] = {
                    "reply_count": len(replies),
                    "replies": list(replies)
                }
        
        return jsonify(stats)
    except Exception as e:
        add_log(f"âŒ è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}", "error")
        return jsonify({"error": str(e)}), 500


@app.route('/api/history', methods=['DELETE'])
def clear_history():
    """æ¸…ç©ºå¯¹è¯å†å²"""
    session_id = request.args.get("session_id", "default")
    if session_id in _conversation_history:
        _conversation_history[session_id] = []
    add_log("ğŸ—‘ï¸ å¯¹è¯å†å²å·²æ¸…ç©º")
    return jsonify({"success": True})


# ==================== å›¾ç‰‡ç®¡ç† API ====================

@app.route('/api/images', methods=['GET'])
def get_images():
    """è·å–å›¾ç‰‡åˆ—è¡¨"""
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}
    images = []
    
    for path in IMAGES_DIR.iterdir():
        if path.suffix.lower() in image_extensions:
            images.append({
                "name": path.name,
                "path": str(path),
                "size": path.stat().st_size,
                "url": f"/api/images/{path.name}"
            })
    
    return jsonify({"images": images, "total": len(images)})


@app.route('/api/images/<filename>')
def serve_image(filename):
    """æä¾›å›¾ç‰‡æ–‡ä»¶"""
    filepath = IMAGES_DIR / filename
    if filepath.exists():
        return send_file(filepath)
    return jsonify({"error": "å›¾ç‰‡ä¸å­˜åœ¨"}), 404


@app.route('/api/images/upload', methods=['POST'])
def upload_image():
    """ä¸Šä¼ å›¾ç‰‡"""
    if 'image' not in request.files:
        return jsonify({"error": "æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶"}), 400
    
    file = request.files['image']
    if file.filename == '':
        return jsonify({"error": "æ–‡ä»¶åä¸ºç©º"}), 400
    
    # ç”Ÿæˆæ–‡ä»¶å
    ext = Path(file.filename).suffix or '.jpg'
    filename = f"{uuid.uuid4().hex[:8]}{ext}"
    filepath = IMAGES_DIR / filename
    
    # é¿å…é‡å
    counter = 1
    while filepath.exists():
        filename = f"{uuid.uuid4().hex[:8]}_{counter}{ext}"
        filepath = IMAGES_DIR / filename
        counter += 1
    
    file.save(filepath)
    add_log(f"âœ… å›¾ç‰‡å·²ä¸Šä¼ : {filename}")
    
    return jsonify({
        "success": True,
        "filename": filename,
        "url": f"/api/images/{filename}"
    })


@app.route('/api/images/<filename>', methods=['DELETE'])
def delete_image(filename):
    """åˆ é™¤å›¾ç‰‡"""
    filepath = IMAGES_DIR / filename
    if filepath.exists():
        os.remove(filepath)
        add_log(f"ğŸ—‘ï¸ å›¾ç‰‡å·²åˆ é™¤: {filename}")
        return jsonify({"success": True})
    return jsonify({"error": "å›¾ç‰‡ä¸å­˜åœ¨"}), 404


if __name__ == "__main__":
    print("=" * 50)
    print("LLM æµ‹è¯• Web æœåŠ¡")
    print("=" * 50)
    
    init_services()
    
    print()
    print("å¯åŠ¨æœåŠ¡: http://localhost:5001")
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("=" * 50)
    
    app.run(host="0.0.0.0", port=5001, debug=False, threaded=True)
